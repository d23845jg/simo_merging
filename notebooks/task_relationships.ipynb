{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL Task Relationships Analysis\n",
    "\n",
    "This notebook analyzes the relationships between different tasks in a multi-task DINOv2 model. It evaluates how task vectors perform when scaled and merged, providing insights into task interference and synergies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import chain\n",
    "\n",
    "# Third-party imports\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "# Local imports\n",
    "from model_merging.aggregator import aggregate_task_vectors\n",
    "from model_merging.eval_utils import (\n",
    "    evaluate_task_vector_at_coef,\n",
    ")\n",
    "from model_merging.task_vectors import MTLTaskVector\n",
    "from model_merging.utils import compute_cosine_similarity_matrix\n",
    "from models.dinov2.mtl.multitasker import MTLDinoV2\n",
    "from training.create_network import *\n",
    "from utils import initialize_wandb\n",
    "\n",
    "# Login to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load the configuration template and set up model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from template\n",
    "env = Environment(loader=FileSystemLoader(\".\"))\n",
    "template = env.get_template(\"config/mtl.yaml.j2\")\n",
    "rendered_yaml = template.render()\n",
    "mm_config = yaml.safe_load(rendered_yaml)\n",
    "\n",
    "# Define model classes\n",
    "model_classes = {\n",
    "    \"dinov2\": MTLDinoV2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb Initialization\n",
    "\n",
    "Initialize Weights & Biases for logging experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wandb for experiment tracking\n",
    "initialize_wandb(\n",
    "    project=mm_config[\"wandb\"][\"project\"],\n",
    "    group=f\"{mm_config['training_params']['network']}\",\n",
    "    job_type=\"task_relationships\",\n",
    "    mode=\"offline\",\n",
    "    config={\n",
    "        \"network\": mm_config[\"model_merging\"][\"network\"],\n",
    "        \"dataset\": mm_config[\"model_merging\"][\"dataset\"],\n",
    "        \"batch_size\": mm_config[\"training_params\"][\"batch_size\"],\n",
    "        \"ft_model_files\": mm_config[\"model_merging\"][\"ft_model_files\"],\n",
    "        \"method\": mm_config[\"model_merging\"][\"method\"],\n",
    "        \"seed\": mm_config[\"training_params\"][\"seed\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Define the tasks and initialize the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tasks for multi-task learning\n",
    "train_tasks = {\n",
    "    \"seg\": {\n",
    "        \"num_classes\": 13,\n",
    "    },\n",
    "    \"depth\": {\n",
    "        \"num_classes\": 1,\n",
    "        \"min_depth\": 0.00,\n",
    "        \"max_depth\": 10.0,\n",
    "    },\n",
    "    \"normal\": {\n",
    "        \"num_classes\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "pt_model = MTLDinoV2(\n",
    "    arch_name=\"vit_base\",\n",
    "    head_tasks=train_tasks,  # Task configurations\n",
    "    head_archs=\"dpt-add_small\",  # Decoder architecture for heads\n",
    "    out_index=[2, 5, 8, 11],  # Output indices from transformer layers\n",
    "    cls_token=True,  # Include class token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Vectors\n",
    "\n",
    "Load fine-tuned models and compute task vectors for each task combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task vectors from fine-tuned model files\n",
    "task_vectors = {\n",
    "    \" + \".join(task_vector.head_tasks.keys()): task_vector\n",
    "    for ft_file in mm_config[\"model_merging\"][\"ft_model_files\"]\n",
    "    for task_vector in [MTLTaskVector(pt_model, ft_file)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display task vector norms\n",
    "[task.title() + f\": {task_vectors[task].norm().item():.2f} \" for task in task_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix between task vectors\n",
    "compute_cosine_similarity_matrix(task_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Task Relationships\n",
    "\n",
    "Evaluate each task vector at different scaling coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate task relationships by scaling coefficients\n",
    "task_relationships = {}\n",
    "for pri_task in itertools.combinations(task_vectors.keys(), 1):\n",
    "    print(f\"\\nMain task: {pri_task}\")\n",
    "    mtl_tv, _ = aggregate_task_vectors(\n",
    "        {task: task_vectors[task] for task in pri_task}, mm_config\n",
    "    )\n",
    "    mtl_tv.tau = dict(chain(*(task_vectors[task].tau.items() for task in task_vectors)))\n",
    "    mtl_tv.head_tasks = dict(\n",
    "        chain(*(task_vectors[task].head_tasks.items() for task in task_vectors))\n",
    "    )\n",
    "\n",
    "    scaling_coef_range = np.linspace(0.0, 1.5, 16)\n",
    "\n",
    "    info = {}\n",
    "    for scaling_coef in scaling_coef_range:\n",
    "        print(f\"Evaluating model with task coefficient: {scaling_coef}\")\n",
    "        info[scaling_coef] = evaluate_task_vector_at_coef(\n",
    "            pt_model,\n",
    "            mtl_tv,\n",
    "            mm_config,\n",
    "            scaling_coef,\n",
    "            use_val_dataset=False,\n",
    "            eval_masks=None,\n",
    "        )\n",
    "    task_relationships[pri_task] = info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Metrics\n",
    "\n",
    "Define baseline metrics for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline task-specific metrics for normalization\n",
    "ts_metrics = {\"seg\": 0.7079, \"depth\": 0.2314, \"normal\": 18.02}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results\n",
    "\n",
    "Visualize the normalized metrics for each primary task across scaling coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each task\n",
    "task_colors = {\"seg\": \"blue\", \"depth\": \"green\", \"normal\": \"red\"}\n",
    "\n",
    "# Define task labels for legend\n",
    "task_labels = {\"seg\": \"Segmentation\", \"depth\": \"Depth\", \"normal\": \"Surface Normal\"}\n",
    "\n",
    "num_tasks = len(task_vectors)\n",
    "fig, axs = plt.subplots(1, num_tasks, figsize=(6 * num_tasks, 6))\n",
    "\n",
    "# Ensure axs is always iterable\n",
    "if num_tasks == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "for i, task in enumerate(task_vectors.keys()):\n",
    "    pri_task = (task,)\n",
    "    info = task_relationships[pri_task]\n",
    "    x_values = sorted(info.keys())  # Ensure x_values are sorted\n",
    "\n",
    "    # Prepare y-values for each task's metrics\n",
    "    metrics = {}\n",
    "    for t in [\"seg\", \"depth\", \"normal\"]:\n",
    "        if t == \"seg\":\n",
    "            # Higher is better for seg\n",
    "            metrics[t] = [info[key][t][\"metric\"][0] / ts_metrics[t] for key in x_values]\n",
    "        else:\n",
    "            # Lower is better for depth and normal\n",
    "            metrics[t] = [ts_metrics[t] / info[key][t][\"metric\"][0] for key in x_values]\n",
    "\n",
    "    # Plot metrics on the i-th subplot\n",
    "    for t in [\"seg\", \"depth\", \"normal\"]:\n",
    "        axs[i].plot(\n",
    "            x_values,\n",
    "            metrics[t],\n",
    "            label=task_labels[t],\n",
    "            marker=\"o\",\n",
    "            color=task_colors[t],\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "    # Add baseline line\n",
    "    axs[i].axhline(y=1.0, color=\"black\", linestyle=\"--\", linewidth=1, label=\"Baseline\")\n",
    "\n",
    "    # Add labels and title\n",
    "    axs[i].set_title(f\"Primary Task: {task}\", fontsize=18)\n",
    "    axs[i].set_xlabel(\"Task Arithmetic Coefficient\", fontsize=16)\n",
    "    axs[i].set_ylabel(\"Normalized Metric\", fontsize=16)\n",
    "    axs[i].set_ylim(0.0, 1.5)  # Adjusted to allow for values above 1.2\n",
    "    axs[i].tick_params(axis=\"x\", labelsize=14)\n",
    "    axs[i].tick_params(axis=\"y\", labelsize=14)\n",
    "    axs[i].legend(fontsize=12, title_fontsize=14)\n",
    "    axs[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Finish logging and clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish Wandb logging\n",
    "wandb.finish(quiet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
